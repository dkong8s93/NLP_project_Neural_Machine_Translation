{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation with Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a project to build Neural Machine Translation (NMT) model to translate human readable dates, \"10th of December, 2018\", into machine readable dates, \"2018-12-10\". This is **very close to the Google Text Normalization Challenge** I once did.\n",
    "\n",
    "Here are some more examples of what we want to achieve:\n",
    "* \"11 November 1993\" ----------> \"1993-11-11\"\n",
    "* \"8 June 1967\" ---------------> \"1967-06-08\"\n",
    "* \"20th of February 1992\" -----> \"1992-02-20\"\n",
    "* \"Sat 10 Jul 2007\" -----------> \"2007-07-10\"\n",
    "* \"Tuesday Sept 11 2011\" ------> \"2011-09-11\"\n",
    "\n",
    "We will use an NLP algorithm called the **Bidirectional LSTM**, a very sophisticated sequence to sequence, seq2seq, to accomplish this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of the dataset, we will generate some sample dates using the faker library (https://github.com/joke2k/faker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from faker import Faker\n",
    "from babel.dates import format_date\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "def load_date():\n",
    "    \n",
    "    # Define the format of the data we like to generate\n",
    "    FORMATS = [\"short\",\n",
    "               \"medium\",\n",
    "               \"long\",\n",
    "               \"full\",\n",
    "               \"full\",\n",
    "               \"full\",\n",
    "               \"full\",\n",
    "               \"full\",\n",
    "               \"full\",\n",
    "               \"full\",\n",
    "               \"full\",\n",
    "               \"full\",\n",
    "               \"full\",\n",
    "               \"d MMM YYY\",\n",
    "               \"d MMMM YYY\",\n",
    "               \"dd MMM YYY\",\n",
    "               \"d MMM, YYY\",\n",
    "               \"d MMMM, YYY\",\n",
    "               \"dd, MMM YYY\",\n",
    "               \"d MM YY\",\n",
    "               \"d MMMM YYY\",\n",
    "               \"MMMM d YYY\",\n",
    "               \"MMMM d, YYY\",\n",
    "               \"dd.MM.YY\"]\n",
    "    \n",
    "    dt = fake.date_object()\n",
    "    \n",
    "    try:\n",
    "        human_readable = format_date(dt, \n",
    "                                     format=random.choice(FORMATS),\n",
    "                                     locale=\"en_US\").lower().replace(\",\",\"\")\n",
    "        machine_readable = dt.isoformat()\n",
    "        \n",
    "    except AttributeError as e:\n",
    "        return None, None, None\n",
    "    \n",
    "    return human_readable, machine_readable, dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(m=10000):\n",
    "    # Load a dataset with 'm' examples\n",
    "    \n",
    "    human_vocab = set()\n",
    "    machine_vocab = set()\n",
    "    dataset = []\n",
    "    Tx = 30 # The maximum length of date format in the dataset\n",
    "    \n",
    "    for i in range(m):\n",
    "        h, m, _ = load_date()\n",
    "        if h is not None:\n",
    "            dataset.append((h,m))\n",
    "            human_vocab.update(tuple(h))\n",
    "            machine_vocab.update(tuple(m))\n",
    "            \n",
    "    human = dict(zip(sorted(human_vocab) + ['<unk>',\"<pad>\"], list(range(len(human_vocab) + 2))))\n",
    "    inv_machine = dict(enumerate(sorted(machine_vocab)))\n",
    "    machine = {v:k for k,v in inv_machine.items()}\n",
    "    \n",
    "    return dataset, human, machine, inv_machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a dataset of 10,000 human readable dates of different formats and their equivalent, standardized, machine readable dates. Below are some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thursday april 6 1995', '1995-04-06'),\n",
       " ('29 nov 1983', '1983-11-29'),\n",
       " ('23 oct 1977', '1977-10-23'),\n",
       " ('04.12.85', '1985-12-04'),\n",
       " ('monday march 4 1974', '1974-03-04'),\n",
       " ('monday may 22 2000', '2000-05-22'),\n",
       " ('17 apr 1987', '1987-04-17'),\n",
       " ('03 oct 2008', '2008-10-03'),\n",
       " ('19 jan 2000', '2000-01-19'),\n",
       " ('19.07.98', '1998-07-19')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 10000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m=m)\n",
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that,\n",
    "* dataset: a list of tuples of (human readable date, machine readable date)\n",
    "* human_vocab: dictionary mapping all characters used in the human readable dates to an integer-valued index\n",
    "* machine_vocab: dictionary mapping all characters used in machine readable dates to an integer-valued index\n",
    "* inv_machine_vocab: the inverse dictionary of machine_vocab, mapping from indices back to characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the data into trainable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to turn words into numbers (word2vec)! Here we map the raw text data into index values using the vocabulary dictionaries loaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def string_to_int(string, length, vocab):\n",
    "    \n",
    "    # Convert all strings in the vocabulary into a list of integers representing the \n",
    "    # positions of the input string's characters in the \"vocab\"\n",
    "    \n",
    "    string = string.lower().replace(\",\",\"\")\n",
    "    \n",
    "    if len(string) > length:\n",
    "        string = string[:length]\n",
    "        \n",
    "    rep = list(map(lambda x: vocab.get(x, \"<unk>\"), string))\n",
    "    \n",
    "    if len(string) < length:\n",
    "        rep += [vocab[\"<pad>\"]] * (length - len(string))\n",
    "        \n",
    "    return rep\n",
    "    \n",
    "def preprocess_data(dataset, human_vocab, machine_vocab, max_length_X, max_length_y):\n",
    "    \n",
    "    X, y = zip(*dataset)\n",
    "    \n",
    "    X = np.array([string_to_int(i,max_length_X,human_vocab) for i in X])\n",
    "    y = [string_to_int(t,max_length_y,machine_vocab) for t in y]\n",
    "    \n",
    "    X_one_hot = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)),X)))\n",
    "    y_one_hot = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), y)))\n",
    "    \n",
    "    return X, np.array(y), X_one_hot, y_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume,\n",
    "* max_length_X = 30: maximum length of human readable dates\n",
    "* max_length_y = 10: maximum of machine readable dates, which is of the format \"YYYY-MM-DD\", which is 10 characters long\n",
    "\n",
    "if any dates in the human readable set is shorter than 30 characters, we pad them with 36's, or the \"<pad\\>\" object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (10000, 30)\n",
      "y shape: (10000, 10)\n",
      "X_one_hot shape: (10000, 30, 37)\n",
      "y_one_hot shape: (10000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "max_length_X = 30\n",
    "max_length_y = 10\n",
    "\n",
    "X, y, X_one_hot, y_one_hot = preprocess_data(dataset, human_vocab, machine_vocab, max_length_X, max_length_y)\n",
    "\n",
    "print(\"X shape: {}\".format(X.shape))\n",
    "print(\"y shape: {}\".format(y.shape))\n",
    "print(\"X_one_hot shape: {}\".format(X_one_hot.shape))\n",
    "print(\"y_one_hot shape: {}\".format(y_one_hot.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show one random example of preprocess training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source date: monday october 8 2007\n",
      "Target date: 2007-10-08\n",
      "====================================================================================================\n",
      "Source after preprocessing (indices): [24 26 25 16 13 34  0 26 15 30 26 14 17 28  0 11  0  5  3  3 10 36 36 36\n",
      " 36 36 36 36 36 36]\n",
      "Target after preprocessing (indices): [3 1 1 8 0 2 1 0 1 9]\n",
      "====================================================================================================\n",
      "Source after preprocessing (one-hot): [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "Target after preprocessing (one-hot): [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0,len(X))\n",
    "print(\"Source date: {}\".format(dataset[idx][0]))\n",
    "print(\"Target date: {}\".format(dataset[idx][1]))\n",
    "print(\"=\" * 100)\n",
    "print(\"Source after preprocessing (indices): {}\".format(X[idx]))\n",
    "print(\"Target after preprocessing (indices): {}\".format(y[idx]))\n",
    "print(\"=\" * 100)\n",
    "print(\"Source after preprocessing (one-hot): {}\".format(X_one_hot[idx]))\n",
    "print(\"Target after preprocessing (one-hot): {}\".format(y_one_hot[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the NMT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is the meat and flesh of this project. We will build a Bidirectional LSTM (BiLSTM) algorithm to translate the dates. \n",
    "\n",
    "We start with the one_step_attention() function, the basic building block of a BiLSTM). Credits to Andrew Ng's course on Deep Learning (https://www.coursera.org/learn/nlp-sequence-models) for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Concatenate, RepeatVector, Activation, Dot\n",
    "import keras.backend as K\n",
    "\n",
    "def custom_softmax(x, axis=1):\n",
    "    \n",
    "    # A custom softmax function \n",
    "    \n",
    "    ndim = K.ndim(x)\n",
    "    if ndim == 2:\n",
    "        return K.softmax(x)\n",
    "    elif ndim > 2:\n",
    "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "        s = K.sum(e, axis=axis, keepdims = True)\n",
    "        return e / s\n",
    "    else:\n",
    "        raise ValueError(\"Cannot apply softmax to a tensor that is 1D\")\n",
    "\n",
    "repeator = RepeatVector(max_length_X)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor_1 = Dense(units=10,activation=\"tanh\")\n",
    "densor_2 = Dense(units=1,activation=\"relu\")\n",
    "activator = Activation(custom_softmax,name=\"attention_weights\") \n",
    "dotor = Dot(axes=1)\n",
    "\n",
    "def one_step_attention(a, s_prev):\n",
    "    # Repeat s_prev to be of shape (m, max_length_X, n_s)\n",
    "    s_prev = repeator(s_prev)\n",
    "    # Concatenate a and s_prev on the last axis\n",
    "    concat = concatenator([a,s_prev])\n",
    "    # Use densor_1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable\n",
    "    energy_intermediate = densor_1(concat)\n",
    "    # Use densor_2 to propagate concat through a small fully-connected neural network to compute the \"energies\" variable\n",
    "    energy = densor_2(energy_intermediate)\n",
    "    # Use \"activator\" on \"energy\" to compute the attention weights \"alphas\"\n",
    "    alphas = activator(energy)\n",
    "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next post-attention LSTM-cell\n",
    "    context = dotor([alphas,a])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the building blocks, we can now build out the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional, LSTM\n",
    "\n",
    "n_a = 32 # hidden state size of the BiLSTM\n",
    "n_s = 64 # hidden state size of the post-attention LSTM\n",
    "post_activation_LSTM_cell = LSTM(n_s,return_state=True)\n",
    "output_layer = Dense(units=len(machine_vocab),activation=custom_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "def model(max_length_X,max_length_y,n_a,n_s,human_vocab_size,machine_vocab_size):\n",
    "    \n",
    "    X = Input(shape=(max_length_X,human_vocab_size))\n",
    "    s0 = Input(shape=(n_s,),name=\"s0\") # Initial hidden state for the decoder LSTM \n",
    "    c0 = Input(shape=(n_s,),name=\"c0\") # Initial hidden state for the decoder LSTM \n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    # Define the pre-attention BiLSTM\n",
    "    a = Bidirectional(LSTM(n_a,return_sequences=True),input_shape=(m,max_length_X,n_a**2))(X)\n",
    "    \n",
    "    # Iterate for max_length_y steps\n",
    "    for t in range(max_length_y):\n",
    "        \n",
    "        # Perform one step of the attention mechanism to get back the context vector \n",
    "        context = one_step_attention(a,s)\n",
    "        \n",
    "        # Apply the post-attention LSTM cell to the \"context\" vector\n",
    "        s, _, c = post_activation_LSTM_cell(context,initial_state=[s,c])\n",
    "        \n",
    "        # Apply Dense layer to the hidden state output of the post-attention LSTM\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Append to the output list\n",
    "        outputs.append(out)\n",
    "    \n",
    "    # Create model instance taking three inputs and returning the list of outputs\n",
    "    model = Model(inputs=[X,s0,c0],outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(max_length_X,max_length_y,n_a,n_s,len(human_vocab),len(machine_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 30, 37)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 30, 64)       17920       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_3 (RepeatVector)  (None, 30, 64)       0           s0[0][0]                         \n",
      "                                                                 lstm_4[0][0]                     \n",
      "                                                                 lstm_4[1][0]                     \n",
      "                                                                 lstm_4[2][0]                     \n",
      "                                                                 lstm_4[3][0]                     \n",
      "                                                                 lstm_4[4][0]                     \n",
      "                                                                 lstm_4[5][0]                     \n",
      "                                                                 lstm_4[6][0]                     \n",
      "                                                                 lstm_4[7][0]                     \n",
      "                                                                 lstm_4[8][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 30, 128)      0           bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_3[0][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_3[1][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_3[2][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_3[3][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_3[4][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_3[5][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_3[6][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_3[7][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_3[8][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_3[9][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 30, 10)       1290        concatenate_3[0][0]              \n",
      "                                                                 concatenate_3[1][0]              \n",
      "                                                                 concatenate_3[2][0]              \n",
      "                                                                 concatenate_3[3][0]              \n",
      "                                                                 concatenate_3[4][0]              \n",
      "                                                                 concatenate_3[5][0]              \n",
      "                                                                 concatenate_3[6][0]              \n",
      "                                                                 concatenate_3[7][0]              \n",
      "                                                                 concatenate_3[8][0]              \n",
      "                                                                 concatenate_3[9][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 30, 1)        11          dense_5[0][0]                    \n",
      "                                                                 dense_5[1][0]                    \n",
      "                                                                 dense_5[2][0]                    \n",
      "                                                                 dense_5[3][0]                    \n",
      "                                                                 dense_5[4][0]                    \n",
      "                                                                 dense_5[5][0]                    \n",
      "                                                                 dense_5[6][0]                    \n",
      "                                                                 dense_5[7][0]                    \n",
      "                                                                 dense_5[8][0]                    \n",
      "                                                                 dense_5[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 30, 1)        0           dense_6[0][0]                    \n",
      "                                                                 dense_6[1][0]                    \n",
      "                                                                 dense_6[2][0]                    \n",
      "                                                                 dense_6[3][0]                    \n",
      "                                                                 dense_6[4][0]                    \n",
      "                                                                 dense_6[5][0]                    \n",
      "                                                                 dense_6[6][0]                    \n",
      "                                                                 dense_6[7][0]                    \n",
      "                                                                 dense_6[8][0]                    \n",
      "                                                                 dense_6[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 1, 64)        0           attention_weights[0][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[4][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[5][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[6][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[7][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[8][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[9][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 64), (None,  33024       dot_2[0][0]                      \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot_2[1][0]                      \n",
      "                                                                 lstm_4[0][0]                     \n",
      "                                                                 lstm_4[0][2]                     \n",
      "                                                                 dot_2[2][0]                      \n",
      "                                                                 lstm_4[1][0]                     \n",
      "                                                                 lstm_4[1][2]                     \n",
      "                                                                 dot_2[3][0]                      \n",
      "                                                                 lstm_4[2][0]                     \n",
      "                                                                 lstm_4[2][2]                     \n",
      "                                                                 dot_2[4][0]                      \n",
      "                                                                 lstm_4[3][0]                     \n",
      "                                                                 lstm_4[3][2]                     \n",
      "                                                                 dot_2[5][0]                      \n",
      "                                                                 lstm_4[4][0]                     \n",
      "                                                                 lstm_4[4][2]                     \n",
      "                                                                 dot_2[6][0]                      \n",
      "                                                                 lstm_4[5][0]                     \n",
      "                                                                 lstm_4[5][2]                     \n",
      "                                                                 dot_2[7][0]                      \n",
      "                                                                 lstm_4[6][0]                     \n",
      "                                                                 lstm_4[6][2]                     \n",
      "                                                                 dot_2[8][0]                      \n",
      "                                                                 lstm_4[7][0]                     \n",
      "                                                                 lstm_4[7][2]                     \n",
      "                                                                 dot_2[9][0]                      \n",
      "                                                                 lstm_4[8][0]                     \n",
      "                                                                 lstm_4[8][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 11)           715         lstm_4[0][0]                     \n",
      "                                                                 lstm_4[1][0]                     \n",
      "                                                                 lstm_4[2][0]                     \n",
      "                                                                 lstm_4[3][0]                     \n",
      "                                                                 lstm_4[4][0]                     \n",
      "                                                                 lstm_4[5][0]                     \n",
      "                                                                 lstm_4[6][0]                     \n",
      "                                                                 lstm_4[7][0]                     \n",
      "                                                                 lstm_4[8][0]                     \n",
      "                                                                 lstm_4[9][0]                     \n",
      "==================================================================================================\n",
      "Total params: 52,960\n",
      "Trainable params: 52,960\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify optimizer, loss function, and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "optimizer = Adam(lr=0.005,beta_1=0.9,beta_2=0.999,decay=0.01)\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=optimizer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = np.zeros((m,n_s))\n",
    "c0 = np.zeros((m,n_s))\n",
    "outputs = list(y_one_hot.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "10000/10000 [==============================] - 6s 591us/step - loss: 9.9291 - dense_9_loss: 2.2779 - dense_9_acc: 0.9492 - dense_9_acc_1: 0.9580 - dense_9_acc_2: 0.4558 - dense_9_acc_3: 0.1428 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.8177 - dense_9_acc_6: 0.2419 - dense_9_acc_7: 1.0000 - dense_9_acc_8: 0.4259 - dense_9_acc_9: 0.1410\n",
      "Epoch 2/30\n",
      "10000/10000 [==============================] - 6s 578us/step - loss: 8.7215 - dense_9_loss: 2.1958 - dense_9_acc: 0.9699 - dense_9_acc_1: 0.9720 - dense_9_acc_2: 0.5301 - dense_9_acc_3: 0.2107 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9245 - dense_9_acc_6: 0.4193 - dense_9_acc_7: 1.0000 - dense_9_acc_8: 0.4366 - dense_9_acc_9: 0.1938\n",
      "Epoch 3/30\n",
      "10000/10000 [==============================] - 7s 723us/step - loss: 7.8893 - dense_9_loss: 2.1174 - dense_9_acc: 0.9733 - dense_9_acc_1: 0.9780 - dense_9_acc_2: 0.5841 - dense_9_acc_3: 0.2899 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9625 - dense_9_acc_6: 0.5504 - dense_9_acc_7: 1.0000 - dense_9_acc_8: 0.4921 - dense_9_acc_9: 0.2197\n",
      "Epoch 4/30\n",
      "10000/10000 [==============================] - 10s 971us/step - loss: 7.2845 - dense_9_loss: 2.0412 - dense_9_acc: 0.9762 - dense_9_acc_1: 0.9809 - dense_9_acc_2: 0.6550 - dense_9_acc_3: 0.3527 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9691 - dense_9_acc_6: 0.6055 - dense_9_acc_7: 1.0000 - dense_9_acc_8: 0.5263 - dense_9_acc_9: 0.2477\n",
      "Epoch 5/30\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 6.7442 - dense_9_loss: 1.9913 - dense_9_acc: 0.9784 - dense_9_acc_1: 0.9817 - dense_9_acc_2: 0.7379 - dense_9_acc_3: 0.4140 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9697 - dense_9_acc_6: 0.6630 - dense_9_acc_7: 1.0000 - dense_9_acc_8: 0.5518 - dense_9_acc_9: 0.2618\n",
      "Epoch 6/30\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 6.2673 - dense_9_loss: 1.9476 - dense_9_acc: 0.9781 - dense_9_acc_1: 0.9820 - dense_9_acc_2: 0.7752 - dense_9_acc_3: 0.4898 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9711 - dense_9_acc_6: 0.7080 - dense_9_acc_7: 1.0000 - dense_9_acc_8: 0.5900 - dense_9_acc_9: 0.2767\n",
      "Epoch 7/30\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 5.8620 - dense_9_loss: 1.8991 - dense_9_acc: 0.9803 - dense_9_acc_1: 0.9829 - dense_9_acc_2: 0.8022 - dense_9_acc_3: 0.5583 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9735 - dense_9_acc_6: 0.7497 - dense_9_acc_7: 1.0000 - dense_9_acc_8: 0.6134 - dense_9_acc_9: 0.2934\n",
      "Epoch 8/30\n",
      "10000/10000 [==============================] - 10s 1ms/step - loss: 5.5094 - dense_9_loss: 1.8377 - dense_9_acc: 0.9807 - dense_9_acc_1: 0.9839 - dense_9_acc_2: 0.8110 - dense_9_acc_3: 0.6046 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9735 - dense_9_acc_6: 0.7776 - dense_9_acc_7: 1.0000 - dense_9_acc_8: 0.6432 - dense_9_acc_9: 0.3151\n",
      "Epoch 9/30\n",
      "10000/10000 [==============================] - 10s 971us/step - loss: 5.1717 - dense_9_loss: 1.7699 - dense_9_acc: 0.9809 - dense_9_acc_1: 0.9841 - dense_9_acc_2: 0.8153 - dense_9_acc_3: 0.6478 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9733 - dense_9_acc_6: 0.7981 - dense_9_acc_7: 1.0000 - dense_9_acc_8: 0.6683 - dense_9_acc_9: 0.3394\n",
      "Epoch 10/30\n",
      "10000/10000 [==============================] - 9s 909us/step - loss: 4.9089 - dense_9_loss: 1.6955 - dense_9_acc: 0.9820 - dense_9_acc_1: 0.9846 - dense_9_acc_2: 0.8208 - dense_9_acc_3: 0.6734 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9737 - dense_9_acc_6: 0.8076 - dense_9_acc_7: 1.0000 - dense_9_acc_8: 0.6904 - dense_9_acc_9: 0.3636\n",
      "Epoch 11/30\n",
      "10000/10000 [==============================] - 8s 831us/step - loss: 4.6569 - dense_9_loss: 1.6196 - dense_9_acc: 0.9827 - dense_9_acc_1: 0.9847 - dense_9_acc_2: 0.8243 - dense_9_acc_3: 0.7024 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9751 - dense_9_acc_6: 0.8204 - dense_9_acc_7: 0.9998 - dense_9_acc_8: 0.7090 - dense_9_acc_9: 0.3893\n",
      "Epoch 12/30\n",
      "10000/10000 [==============================] - 8s 839us/step - loss: 4.4527 - dense_9_loss: 1.5517 - dense_9_acc: 0.9827 - dense_9_acc_1: 0.9848 - dense_9_acc_2: 0.8256 - dense_9_acc_3: 0.7196 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9768 - dense_9_acc_6: 0.8230 - dense_9_acc_7: 1.0000 - dense_9_acc_8: 0.7239 - dense_9_acc_9: 0.4065\n",
      "Epoch 13/30\n",
      "10000/10000 [==============================] - 9s 873us/step - loss: 4.2443 - dense_9_loss: 1.4790 - dense_9_acc: 0.9828 - dense_9_acc_1: 0.9858 - dense_9_acc_2: 0.8335 - dense_9_acc_3: 0.7387 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9770 - dense_9_acc_6: 0.8290 - dense_9_acc_7: 1.0000 - dense_9_acc_8: 0.7404 - dense_9_acc_9: 0.4341\n",
      "Epoch 14/30\n",
      "10000/10000 [==============================] - 8s 832us/step - loss: 4.0735 - dense_9_loss: 1.4078 - dense_9_acc: 0.9838 - dense_9_acc_1: 0.9858 - dense_9_acc_2: 0.8345 - dense_9_acc_3: 0.7569 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9779 - dense_9_acc_6: 0.8324 - dense_9_acc_7: 1.0000 - dense_9_acc_8: 0.7512 - dense_9_acc_9: 0.4601\n",
      "Epoch 15/30\n",
      "10000/10000 [==============================] - 8s 821us/step - loss: 3.9133 - dense_9_loss: 1.3448 - dense_9_acc: 0.9843 - dense_9_acc_1: 0.9863 - dense_9_acc_2: 0.8361 - dense_9_acc_3: 0.7751 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9767 - dense_9_acc_6: 0.8362 - dense_9_acc_7: 1.0000 - dense_9_acc_8: 0.7668 - dense_9_acc_9: 0.48576s - loss: 4.0010 - dense_9_loss: 1.3803 - dense_9_acc: 0.9840 - dense_9_acc_1: 0.9860 - dense_9_acc_2: 0.83\n",
      "Epoch 16/30\n",
      "10000/10000 [==============================] - 8s 803us/step - loss: 3.7738 - dense_9_loss: 1.2823 - dense_9_acc: 0.9845 - dense_9_acc_1: 0.9867 - dense_9_acc_2: 0.8414 - dense_9_acc_3: 0.7890 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9780 - dense_9_acc_6: 0.8361 - dense_9_acc_7: 1.0000 - dense_9_acc_8: 0.7723 - dense_9_acc_9: 0.5073\n",
      "Epoch 17/30\n",
      "10000/10000 [==============================] - 8s 805us/step - loss: 3.6417 - dense_9_loss: 1.2262 - dense_9_acc: 0.9857 - dense_9_acc_1: 0.9867 - dense_9_acc_2: 0.8418 - dense_9_acc_3: 0.8031 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9774 - dense_9_acc_6: 0.8410 - dense_9_acc_7: 1.0000 - dense_9_acc_8: 0.7793 - dense_9_acc_9: 0.5326\n",
      "Epoch 18/30\n",
      "10000/10000 [==============================] - 8s 801us/step - loss: 3.5246 - dense_9_loss: 1.1781 - dense_9_acc: 0.9855 - dense_9_acc_1: 0.9870 - dense_9_acc_2: 0.8452 - dense_9_acc_3: 0.8164 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9790 - dense_9_acc_6: 0.8435 - dense_9_acc_7: 0.9999 - dense_9_acc_8: 0.7876 - dense_9_acc_9: 0.5503\n",
      "Epoch 19/30\n",
      "10000/10000 [==============================] - 8s 789us/step - loss: 3.4240 - dense_9_loss: 1.1335 - dense_9_acc: 0.9864 - dense_9_acc_1: 0.9877 - dense_9_acc_2: 0.8462 - dense_9_acc_3: 0.8332 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9788 - dense_9_acc_6: 0.8439 - dense_9_acc_7: 1.0000 - dense_9_acc_8: 0.7886 - dense_9_acc_9: 0.5672\n",
      "Epoch 20/30\n",
      "10000/10000 [==============================] - 8s 813us/step - loss: 3.3223 - dense_9_loss: 1.0925 - dense_9_acc: 0.9865 - dense_9_acc_1: 0.9873 - dense_9_acc_2: 0.8463 - dense_9_acc_3: 0.8402 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9779 - dense_9_acc_6: 0.8470 - dense_9_acc_7: 0.9998 - dense_9_acc_8: 0.7993 - dense_9_acc_9: 0.5859\n",
      "Epoch 21/30\n",
      "10000/10000 [==============================] - 8s 806us/step - loss: 3.2281 - dense_9_loss: 1.0532 - dense_9_acc: 0.9873 - dense_9_acc_1: 0.9879 - dense_9_acc_2: 0.8431 - dense_9_acc_3: 0.8494 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9797 - dense_9_acc_6: 0.8505 - dense_9_acc_7: 0.9998 - dense_9_acc_8: 0.8029 - dense_9_acc_9: 0.6042\n",
      "Epoch 22/30\n",
      "10000/10000 [==============================] - 8s 814us/step - loss: 3.1459 - dense_9_loss: 1.0177 - dense_9_acc: 0.9870 - dense_9_acc_1: 0.9881 - dense_9_acc_2: 0.8471 - dense_9_acc_3: 0.8586 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9791 - dense_9_acc_6: 0.8546 - dense_9_acc_7: 1.0000 - dense_9_acc_8: 0.8082 - dense_9_acc_9: 0.6211\n",
      "Epoch 23/30\n",
      "10000/10000 [==============================] - 8s 798us/step - loss: 3.0817 - dense_9_loss: 0.9887 - dense_9_acc: 0.9869 - dense_9_acc_1: 0.9881 - dense_9_acc_2: 0.8495 - dense_9_acc_3: 0.8642 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9786 - dense_9_acc_6: 0.8497 - dense_9_acc_7: 1.0000 - dense_9_acc_8: 0.8133 - dense_9_acc_9: 0.6320\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 8s 783us/step - loss: 2.9944 - dense_9_loss: 0.9543 - dense_9_acc: 0.9877 - dense_9_acc_1: 0.9885 - dense_9_acc_2: 0.8474 - dense_9_acc_3: 0.8727 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9801 - dense_9_acc_6: 0.8587 - dense_9_acc_7: 0.9999 - dense_9_acc_8: 0.8148 - dense_9_acc_9: 0.6474\n",
      "Epoch 25/30\n",
      "10000/10000 [==============================] - 8s 785us/step - loss: 2.9298 - dense_9_loss: 0.9280 - dense_9_acc: 0.9879 - dense_9_acc_1: 0.9890 - dense_9_acc_2: 0.8485 - dense_9_acc_3: 0.8797 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9796 - dense_9_acc_6: 0.8596 - dense_9_acc_7: 0.9999 - dense_9_acc_8: 0.8208 - dense_9_acc_9: 0.6573\n",
      "Epoch 26/30\n",
      "10000/10000 [==============================] - 8s 783us/step - loss: 2.8707 - dense_9_loss: 0.9002 - dense_9_acc: 0.9881 - dense_9_acc_1: 0.9889 - dense_9_acc_2: 0.8500 - dense_9_acc_3: 0.8821 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9797 - dense_9_acc_6: 0.8612 - dense_9_acc_7: 0.9999 - dense_9_acc_8: 0.8191 - dense_9_acc_9: 0.6716\n",
      "Epoch 27/30\n",
      "10000/10000 [==============================] - 8s 768us/step - loss: 2.8108 - dense_9_loss: 0.8754 - dense_9_acc: 0.9879 - dense_9_acc_1: 0.9894 - dense_9_acc_2: 0.8517 - dense_9_acc_3: 0.8890 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9816 - dense_9_acc_6: 0.8634 - dense_9_acc_7: 0.9999 - dense_9_acc_8: 0.8234 - dense_9_acc_9: 0.6835\n",
      "Epoch 28/30\n",
      "10000/10000 [==============================] - 8s 775us/step - loss: 2.7555 - dense_9_loss: 0.8528 - dense_9_acc: 0.9887 - dense_9_acc_1: 0.9900 - dense_9_acc_2: 0.8504 - dense_9_acc_3: 0.8952 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9812 - dense_9_acc_6: 0.8639 - dense_9_acc_7: 1.0000 - dense_9_acc_8: 0.8250 - dense_9_acc_9: 0.6937\n",
      "Epoch 29/30\n",
      "10000/10000 [==============================] - 8s 780us/step - loss: 2.7040 - dense_9_loss: 0.8321 - dense_9_acc: 0.9887 - dense_9_acc_1: 0.9892 - dense_9_acc_2: 0.8524 - dense_9_acc_3: 0.8979 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9815 - dense_9_acc_6: 0.8658 - dense_9_acc_7: 1.0000 - dense_9_acc_8: 0.8246 - dense_9_acc_9: 0.7007\n",
      "Epoch 30/30\n",
      "10000/10000 [==============================] - 8s 785us/step - loss: 2.6574 - dense_9_loss: 0.8137 - dense_9_acc: 0.9889 - dense_9_acc_1: 0.9904 - dense_9_acc_2: 0.8540 - dense_9_acc_3: 0.9011 - dense_9_acc_4: 1.0000 - dense_9_acc_5: 0.9817 - dense_9_acc_6: 0.8700 - dense_9_acc_7: 1.0000 - dense_9_acc_8: 0.8280 - dense_9_acc_9: 0.7086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18986ef9550>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X_one_hot,s0,c0],outputs,epochs=30,batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test our model on some custom data. Feel free to change the custom data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:  11 November 1993\n",
      "output:  1993-11-11\n",
      "==================================================\n",
      "source:  8 June 1967\n",
      "output:  1976-06-07\n",
      "==================================================\n",
      "source:  20th of February 1992\n",
      "output:  1992-02-20\n",
      "==================================================\n",
      "source:  Sat 10 Jul 2007\n",
      "output:  2007-07-10\n",
      "==================================================\n",
      "source:  Tuesday Sept 11 2011\n",
      "output:  2011-09-11\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "test_samples = [\"11 November 1993\",\"8 June 1967\",\"20th of February 1992\",\"Sat 10 Jul 2007\",\"Tuesday Sept 11 2011\"]\n",
    "\n",
    "for sample in test_samples:\n",
    "    \n",
    "    source = string_to_int(sample,max_length_X,human_vocab)\n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)),source)))\n",
    "    source = np.array([source])\n",
    "    prediction = model.predict([source,s0,c0])\n",
    "    prediction = np.argmax(prediction,axis=-1)\n",
    "    output = [inv_machine_vocab[int(entry)] for entry in prediction]\n",
    "    \n",
    "    print(\"source: \", sample)\n",
    "    print(\"output: \", \"\".join(output))\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is not 100% on every test sample. However we can train it for more epochs and it should do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
